{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implemantar la clase con el modelo base de Logistic Classifier\n",
    "class MLP:\n",
    "    def __init__(self, seq_max_len, state_size, vocab_size, num_class):\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.state_size = state_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "    def build (self):\n",
    "        self.x = tf.placeholder (shape=[None, self.seq_max_len], dtype=tf.float32)\n",
    "        x_one_hot_en = tf.one_hot(self.x, self.vocab_size)\n",
    "        \n",
    "        self.y = tf.placeholder (shape=[None], dtype=tf.float32)\n",
    "        y_one_hot_en = tf.one_hot(self.y, self.num_classes)\n",
    "        self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "        \n",
    "        weights = {\n",
    "            'layer_0': tf.Variable(tf.random_normal ([self.seq_max_len*self.vocab_size, self.num_classes])),\n",
    "            'layer_1': tf.Variable(tf.random_normal ([self.state_size, self.state_size ])),\n",
    "            'layer_2': tf.Variable(tf.random_normal ([self.seq_max_len, self.num_classes]))\n",
    "        }\n",
    "      # y = wx + b\n",
    "        biases = {\n",
    "            'layer_0':  tf.Variable(tf.random_normal ([self.state_size])),\n",
    "            'layer_1':  tf.Variable(tf.random_normal ([self.state_size])),\n",
    "            'layer_2':  tf.Variable(tf.random_normal ([self.num_classes]))\n",
    "        }\n",
    "        \n",
    "        x_input = tf.reshape(x_one_hot_en, [-1, self.seq_max_len*selft.vocab_size])\n",
    "        hidden_layer = tf.matmul(x_input, weights['layer_0']) + biases['layer_0']\n",
    "        hidden_layer = tf.nn.sigmoid(hidden_layer)\n",
    "        hidden_layer = tf.matmul(hidden_layer, weights['layer_1']) + biases['layer_1']\n",
    "        hidden_layer = tf.nn.sigmoid(hidden_layer)\n",
    "        self.logits = tf.matmul(hidden_layer, weights['layer_2']) + biases['layer_2']\n",
    "        #Normalizar los valores que arroja la red\n",
    "        self.probs = tf.nn.softmax(self.logits, axis=1)\n",
    "        \n",
    "        self.correct_preds = tf.equal (tf.argmax(self.probs, axis =1), tf.argmax(self.y_one_hot_en, axis=1))\n",
    "        self.precission = tf.reduce_mean(correct_preds)\n",
    "        return \n",
    "    \n",
    "    def step_training (self, learning_rate=0.1):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_one_hot_en, logits=self.logits))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        return loss, optimizer\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "deep_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
