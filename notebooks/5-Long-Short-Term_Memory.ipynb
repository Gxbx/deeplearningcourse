{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implemantar la clase con el modelo base de Logistic Classifier\n",
    "class LSTM:\n",
    "    def __init__(self, seq_max_len, state_size, vocab_size, num_class):\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.state_size = state_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def build (self):\n",
    "        self.x = tf.placeholder (shape=[None, self.seq_max_len], dtype=tf.float32)\n",
    "        x_one_hot_en = tf.one_hot(self.x, self.vocab_size)\n",
    "        \n",
    "        #Agregamos la representaci√≥n temporal del LSTM\n",
    "        rnn_input = tf.unstack (x_one_hot_en, axis=1)\n",
    "        \n",
    "        self.y = tf.placeholder (shape=[None], dtype=tf.float32)\n",
    "        y_one_hot_en = tf.one_hot(self.y, self.num_classes)\n",
    "        \n",
    "        self.batch_size = tf.placeholder(tf.int32, [], name='batch_size')\n",
    "        \n",
    "        weights = {\n",
    "            'layer_0': tf.Variable(tf.random_normal ([self.state_size, 280])),\n",
    "            'layer_1': tf.Variable(tf.random_normal ([280, self.num_classes ])),\n",
    "        }\n",
    "      # y = wx + b\n",
    "        biases = {\n",
    "            'layer_0':  tf.Variable(tf.random_normal ([280])),\n",
    "            'layer_1':  tf.Variable(tf.random_normal ([280, self.num_classes])),\n",
    "        }\n",
    "        \n",
    "        init_state = tf.zeros([self.batch_size, self.state_size]) #debe tener un estado inicial\n",
    "        #BasicLSTMCell()\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(self.state_size)\n",
    "        self.outputs, self.final_state =  tf.contrib.rnn.static_rnn(cell, rnn_input, dtype=tf.float32)\n",
    "        \n",
    "        output = self.outputs[-1]\n",
    "        \n",
    "        \n",
    "        hidden_layer = tf.matmul(output, weights['layer_0']) + biases['layer_0']\n",
    "        hidden_layer = tf.nn.tanh(hidden_layer)\n",
    "\n",
    "        self.logits = tf.matmul(hidden_layer, weights['layer_1']) + biases['layer_1']\n",
    "        #Normalizar los valores que arroja la red\n",
    "        self.probs = tf.nn.softmax(self.logits, axis=1)\n",
    "        \n",
    "        self.correct_preds = tf.equal (tf.argmax(self.probs, axis =1), tf.argmax(self.y_one_hot_en, axis=1))\n",
    "        self.precission = tf.reduce_mean(correct_preds)\n",
    "        return \n",
    "    \n",
    "    def step_training (self, learning_rate=0.1):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_one_hot_en, logits=self.logits))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "        return loss, optimizer\n",
    "        \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_env",
   "language": "python",
   "name": "deep_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
